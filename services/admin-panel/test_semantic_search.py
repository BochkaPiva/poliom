#!/usr/bin/env python3
"""
–¢–µ—Å—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –æ–±—Ö–æ–¥–æ–º –ø—Ä–æ–±–ª–µ–º—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
"""

import os
import sys
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
from sqlalchemy import text, create_engine
from sentence_transformers import SentenceTransformer

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
current_dir = Path(__file__).parent
load_dotenv(current_dir / '.env.local')

# –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
DATABASE_URL = os.getenv("DATABASE_URL")
engine = create_engine(DATABASE_URL)

def load_embedding_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    print("ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    model = SentenceTransformer('cointegrated/rubert-tiny2')
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    return model

def text_search(query, limit=5):
    """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
    print(f"üîç –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫: '{query}'")
    print("-" * 60)
    
    with engine.connect() as conn:
        result = conn.execute(
            text("""
                SELECT 
                    dc.document_id,
                    d.title,
                    dc.chunk_index,
                    dc.content,
                    dc.content_length
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE LOWER(dc.content) LIKE LOWER(:query)
                ORDER BY dc.document_id, dc.chunk_index
                LIMIT :limit
            """),
            {"query": f"%{query}%", "limit": limit}
        )
        
        results = result.fetchall()
        
        if not results:
            print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return []
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print()
        
        for i, (doc_id, title, chunk_idx, content, length) in enumerate(results, 1):
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            query_lower = query.lower()
            content_lower = content.lower()
            pos = content_lower.find(query_lower)
            
            if pos != -1:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
                start = max(0, pos - 100)
                end = min(len(content), pos + len(query) + 100)
                context = content[start:end]
                if start > 0:
                    context = "..." + context
                if end < len(content):
                    context = context + "..."
            else:
                context = content[:200] + "..."
            
            print(f"{i}. –î–æ–∫—É–º–µ–Ω—Ç: {title} (ID: {doc_id})")
            print(f"   –ß–∞–Ω–∫: {chunk_idx}, –î–ª–∏–Ω–∞: {length}")
            print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}")
            print()
        
        return results

def semantic_search_simulation(query, model, limit=5):
    """–°–∏–º—É–ª—è—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –±–µ–∑ —á—Ç–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –ë–î"""
    print(f"üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (—Å–∏–º—É–ª—è—Ü–∏—è): '{query}'")
    print("-" * 60)
    
    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = model.encode([query])[0]
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(query_embedding)})")
    
    with engine.connect() as conn:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        result = conn.execute(
            text("""
                SELECT 
                    dc.document_id,
                    d.title,
                    dc.chunk_index,
                    dc.content,
                    dc.content_length
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.processing_status = 'completed'
                ORDER BY dc.document_id, dc.chunk_index
            """)
        )
        
        chunks = result.fetchall()
        print(f"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(chunks)} —á–∞–Ω–∫–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –æ–Ω–∏ —É–∂–µ –µ—Å—Ç—å –≤ –ë–î)
        chunk_texts = [chunk[3] for chunk in chunks]  # content
        chunk_embeddings = model.encode(chunk_texts)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = []
        for i, chunk_emb in enumerate(chunk_embeddings):
            similarity = np.dot(query_embedding, chunk_emb) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb)
            )
            similarities.append((similarity, chunks[i]))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x[0], reverse=True)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print()
        
        for i, (similarity, (doc_id, title, chunk_idx, content, length)) in enumerate(similarities[:limit], 1):
            preview = content[:300] + "..." if len(content) > 300 else content
            
            print(f"{i}. –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f}")
            print(f"   –î–æ–∫—É–º–µ–Ω—Ç: {title} (ID: {doc_id})")
            print(f"   –ß–∞–Ω–∫: {chunk_idx}, –î–ª–∏–Ω–∞: {length}")
            print(f"   –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {preview}")
            print()
        
        return similarities[:limit]

def test_search_queries():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã"""
    print("üéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û–ò–°–ö–û–í–´–• –ó–ê–ü–†–û–°–û–í")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = load_embedding_model()
    print()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    queries = [
        "–ø—Ä–µ–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤",
        "—Ä–∞–∑–º–µ—Ä –ø—Ä–µ–º–∏–∏",
        "—É—Å–ª–æ–≤–∏—è –≤—ã–ø–ª–∞—Ç—ã",
        "–æ—Ç—á–µ—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥",
        "–≥—Ä–µ–π–¥ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏"
    ]
    
    for query in queries:
        print("=" * 60)
        
        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
        text_results = text_search(query, limit=3)
        print()
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (—Å–∏–º—É–ª—è—Ü–∏—è)
        semantic_results = semantic_search_simulation(query, model, limit=3)
        print()
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print(f"   –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫: {len(text_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        print(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: {len(semantic_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        if semantic_results:
            best_similarity = semantic_results[0][0]
            print(f"   –õ—É—á—à–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {best_similarity:.4f}")
        
        print()

def main():
    print("üîç –¢–ï–°–¢ –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –ü–û–ò–°–ö–ê")
    print("=" * 60)
    print()
    
    try:
        test_search_queries()
        print("üéâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 