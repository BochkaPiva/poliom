#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏
"""

import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ services
services_path = Path(__file__).parent.parent
sys.path.append(str(services_path))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
from dotenv import load_dotenv
load_dotenv('.env.local')

from sqlalchemy.orm import sessionmaker
from shared.models.database import engine
from shared.models import Document, DocumentChunk
from shared.utils.document_processor import DocumentProcessor
from shared.utils.embeddings import EmbeddingService
from datetime import datetime

# –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def analyze_current_chunks():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–µ —á–∞–Ω–∫–∏"""
    print("üìä –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–∏—Ö —á–∞–Ω–∫–æ–≤...\n")
    
    db = SessionLocal()
    try:
        chunks = db.query(DocumentChunk).all()
        
        if not chunks:
            print("‚ùå –ß–∞–Ω–∫–æ–≤ –≤ –±–∞–∑–µ –Ω–µ—Ç")
            return
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º
        sizes = [chunk.content_length for chunk in chunks]
        sizes.sort()
        
        print(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º —á–∞–Ω–∫–æ–≤:")
        print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(sizes)}")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min(sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max(sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {sum(sizes) // len(sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {sizes[len(sizes)//2]} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º
        small = len([s for s in sizes if s < 100])
        medium = len([s for s in sizes if 100 <= s < 500])
        large = len([s for s in sizes if s >= 500])
        
        print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º:")
        print(f"   –ú–∞–ª–µ–Ω—å–∫–∏–µ (<100 —Å–∏–º–≤–æ–ª–æ–≤): {small} ({small/len(sizes)*100:.1f}%)")
        print(f"   –°—Ä–µ–¥–Ω–∏–µ (100-500 —Å–∏–º–≤–æ–ª–æ–≤): {medium} ({medium/len(sizes)*100:.1f}%)")
        print(f"   –ë–æ–ª—å—à–∏–µ (>=500 —Å–∏–º–≤–æ–ª–æ–≤): {large} ({large/len(sizes)*100:.1f}%)")
        
        # –ü—Ä–∏–º–µ—Ä—ã –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤
        small_chunks = [chunk for chunk in chunks if chunk.content_length < 50][:5]
        if small_chunks:
            print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤:")
            for chunk in small_chunks:
                print(f"   –ß–∞–Ω–∫ {chunk.chunk_index}: {chunk.content_length} —Å–∏–º–≤–æ–ª–æ–≤ - '{chunk.content}'")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
    finally:
        db.close()

def recreate_chunks_for_document(document_id: int, chunk_size: int = 1000, overlap: int = 200):
    """–ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    print(f"\nüîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}...")
    print(f"   –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {chunk_size} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {overlap} —Å–∏–º–≤–æ–ª–æ–≤")
    
    db = SessionLocal()
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        document = db.query(Document).filter(Document.id == document_id).first()
        if not document:
            print(f"‚ùå –î–æ–∫—É–º–µ–Ω—Ç {document_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        
        print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {document.original_filename}")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏
        old_chunks_count = db.query(DocumentChunk).filter(DocumentChunk.document_id == document_id).count()
        print(f"üóëÔ∏è –£–¥–∞–ª—è–µ–º {old_chunks_count} —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤...")
        
        db.query(DocumentChunk).filter(DocumentChunk.document_id == document_id).delete()
        db.commit()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        processor = DocumentProcessor()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        print("üìñ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞...")
        text_content = processor.extract_text(document.file_path)
        if not text_content or not text_content.strip():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            return False
        
        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        print("‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏...")
        chunks = processor.split_into_chunks(text_content, chunk_size=chunk_size, overlap=overlap)
        if not chunks:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏")
            return False
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —á–∞–Ω–∫–∏
        new_sizes = [len(chunk) for chunk in chunks]
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤:")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min(new_sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max(new_sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {sum(new_sizes) // len(new_sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print("üß† –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        embedding_service = EmbeddingService()
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        created_chunks = []
        for i, chunk_text in enumerate(chunks):
            try:
                print(f"   –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫ {i+1}/{len(chunks)}...", end='\r')
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞
                embedding = embedding_service.get_embedding(chunk_text)
                
                # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
                chunk = DocumentChunk(
                    document_id=document.id,
                    chunk_index=i,
                    content=chunk_text,
                    content_length=len(chunk_text),
                    embedding=embedding,
                    created_at=datetime.utcnow()
                )
                
                db.add(chunk)
                created_chunks.append(chunk)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 50 —á–∞–Ω–∫–æ–≤
                if (i + 1) % 50 == 0:
                    db.commit()
                    print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {i+1} —á–∞–Ω–∫–æ–≤...")
                
            except Exception as e:
                print(f"\n‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–∞ {i}: {str(e)}")
                continue
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        db.commit()
        print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(created_chunks)} —á–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document.chunks_count = len(created_chunks)
        document.updated_at = datetime.utcnow()
        db.commit()
        
        print(f"üéâ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω!")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–æ–≤: {e}")
        return False
    finally:
        db.close()

def test_new_search():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ —Å –Ω–æ–≤—ã–º–∏ —á–∞–Ω–∫–∞–º–∏"""
    print("\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ —Å –Ω–æ–≤—ã–º–∏ —á–∞–Ω–∫–∞–º–∏...\n")
    
    db = SessionLocal()
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embedding_service = EmbeddingService()
        
        test_queries = [
            "–æ–ø–ª–∞—Ç–∞ —Ç—Ä—É–¥–∞",
            "–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞", 
            "—Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä",
            "–æ—Ç–ø—É—Å–∫",
            "–ø—Ä–µ–º–∏—è"
        ]
        
        for query in test_queries:
            print(f"üîç –ü–æ–∏—Å–∫: '{query}'")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = embedding_service.get_embedding(query)
            if not query_embedding:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞")
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
            chunks = db.query(DocumentChunk).filter(DocumentChunk.embedding.isnot(None)).all()
            
            if not chunks:
                print("‚ùå –ù–µ—Ç —á–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏")
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            similarities = []
            for chunk in chunks:
                try:
                    similarity = embedding_service.calculate_similarity(query_embedding, chunk.embedding)
                    similarities.append((chunk, similarity))
                except Exception as e:
                    continue
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            for i, (chunk, similarity) in enumerate(similarities[:3]):
                document = db.query(Document).filter(Document.id == chunk.document_id).first()
                
                print(f"   #{i+1} –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.3f}")
                print(f"      üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {document.original_filename if document else f'ID {chunk.document_id}'}")
                print(f"      üß© –ß–∞–Ω–∫ {chunk.chunk_index}: {len(chunk.content)} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"      üìù –¢–µ–∫—Å—Ç: {chunk.content[:150]}...")
            
            print("-" * 60)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–∞: {e}")
    finally:
        db.close()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –†–ê–ó–ë–ò–ï–ù–ò–Ø –î–û–ö–£–ú–ï–ù–¢–û–í –ù–ê –ß–ê–ù–ö–ò\n")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–µ —á–∞–Ω–∫–∏
    analyze_current_chunks()
    
    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    print("\n" + "="*60)
    print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ë—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —á–∞–Ω–∫–∏ –∏ —Å–æ–∑–¥–∞–Ω—ã –Ω–æ–≤—ã–µ!")
    print("–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.")
    
    response = input("\n–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/N): ").strip().lower()
    if response != 'y':
        print("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
        return
    
    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ ID=1
    success = recreate_chunks_for_document(
        document_id=1,
        chunk_size=1000,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        overlap=200       # –£–º–µ–Ω—å—à–∞–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
    )
    
    if success:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫
        test_new_search()
        
        print("\n" + "="*60)
        print("üéâ –ß–ê–ù–ö–ò –£–°–ü–ï–®–ù–û –ü–ï–†–ï–°–û–ó–î–ê–ù–´!")
        print("‚úÖ –¢–µ–ø–µ—Ä—å –ø–æ–∏—Å–∫ –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–º–Ω–æ–≥–æ –ª—É—á—à–µ")
        print("‚úÖ –ß–∞–Ω–∫–∏ –∏–º–µ—é—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    else:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏")

if __name__ == "__main__":
    main() 