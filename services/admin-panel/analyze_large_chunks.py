#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –±–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ services
services_path = Path(__file__).parent.parent
sys.path.append(str(services_path))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
from dotenv import load_dotenv
load_dotenv('.env.local')

from shared.models.database import SessionLocal
from shared.models.document import DocumentChunk, Document

def analyze_chunk_sizes():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤"""
    print("üîç –ê–ù–ê–õ–ò–ó –†–ê–ó–ú–ï–†–û–í –ß–ê–ù–ö–û–í\n")
    
    db = SessionLocal()
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
        chunks = db.query(DocumentChunk).all()
        
        if not chunks:
            print("‚ùå –ß–∞–Ω–∫–æ–≤ –≤ –±–∞–∑–µ –Ω–µ—Ç")
            return
        
        print(f"üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã
        sizes = [chunk.content_length for chunk in chunks]
        sizes.sort()
        
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤:")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π: {min(sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π: {max(sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –°—Ä–µ–¥–Ω–∏–π: {sum(sizes) // len(sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–µ–¥–∏–∞–Ω–Ω—ã–π: {sizes[len(sizes)//2]} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º
        tiny = len([s for s in sizes if s < 10])
        small = len([s for s in sizes if 10 <= s < 100])
        medium = len([s for s in sizes if 100 <= s < 500])
        large = len([s for s in sizes if s >= 500])
        
        print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        print(f"   –ö—Ä–æ—à–µ—á–Ω—ã–µ (<10): {tiny} ({tiny/len(sizes)*100:.1f}%)")
        print(f"   –ú–∞–ª–µ–Ω—å–∫–∏–µ (10-100): {small} ({small/len(sizes)*100:.1f}%)")
        print(f"   –°—Ä–µ–¥–Ω–∏–µ (100-500): {medium} ({medium/len(sizes)*100:.1f}%)")
        print(f"   –ë–æ–ª—å—à–∏–µ (>=500): {large} ({large/len(sizes)*100:.1f}%)")
        
        return chunks
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return []
    finally:
        db.close()

def show_large_chunks():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏"""
    print("\nüîç –ê–ù–ê–õ–ò–ó –ë–û–õ–¨–®–ò–• –ß–ê–ù–ö–û–í\n")
    
    db = SessionLocal()
    try:
        # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–º –±–æ–ª—å—à–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
        large_chunks = db.query(DocumentChunk).filter(
            DocumentChunk.content_length >= 500
        ).order_by(DocumentChunk.content_length.desc()).limit(5).all()
        
        if not large_chunks:
            print("‚ùå –ë–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return
        
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(large_chunks)} –±–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤:")
        
        for i, chunk in enumerate(large_chunks):
            document = db.query(Document).filter(Document.id == chunk.document_id).first()
            
            print(f"\nüß© –ß–ê–ù–ö #{i+1}")
            print(f"   ID: {chunk.id}")
            print(f"   –î–æ–∫—É–º–µ–Ω—Ç: {document.original_filename if document else f'ID {chunk.document_id}'}")
            print(f"   –ò–Ω–¥–µ–∫—Å: {chunk.chunk_index}")
            print(f"   –†–∞–∑–º–µ—Ä: {chunk.content_length} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –°–æ–¥–µ—Ä–∂–∏–º–æ–µ:")
            print(f"   {'-'*60}")
            print(f"   {chunk.content[:300]}...")
            if len(chunk.content) > 300:
                print(f"   [... –µ—â–µ {len(chunk.content) - 300} —Å–∏–º–≤–æ–ª–æ–≤]")
            print(f"   {'-'*60}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        db.close()

def show_small_chunks():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏"""
    print("\nüîç –ê–ù–ê–õ–ò–ó –ú–ê–õ–ï–ù–¨–ö–ò–• –ß–ê–ù–ö–û–í\n")
    
    db = SessionLocal()
    try:
        # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–º –º–µ–Ω—å—à–µ 50 —Å–∏–º–≤–æ–ª–æ–≤
        small_chunks = db.query(DocumentChunk).filter(
            DocumentChunk.content_length < 50
        ).order_by(DocumentChunk.chunk_index).limit(10).all()
        
        if not small_chunks:
            print("‚ùå –ú–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return
        
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(small_chunks)} –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤:")
        
        for i, chunk in enumerate(small_chunks):
            document = db.query(Document).filter(Document.id == chunk.document_id).first()
            
            print(f"\nüß© –ß–ê–ù–ö #{i+1}")
            print(f"   ID: {chunk.id}")
            print(f"   –î–æ–∫—É–º–µ–Ω—Ç: {document.original_filename if document else f'ID {chunk.document_id}'}")
            print(f"   –ò–Ω–¥–µ–∫—Å: {chunk.chunk_index}")
            print(f"   –†–∞–∑–º–µ—Ä: {chunk.content_length} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: '{chunk.content}'")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        db.close()

def analyze_sequential_chunks():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏"""
    print("\nüîç –ê–ù–ê–õ–ò–ó –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–• –ß–ê–ù–ö–û–í\n")
    
    db = SessionLocal()
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —á–∞–Ω–∫–æ–≤ –ø–æ –ø–æ—Ä—è–¥–∫—É
        chunks = db.query(DocumentChunk).order_by(
            DocumentChunk.document_id, 
            DocumentChunk.chunk_index
        ).limit(10).all()
        
        if not chunks:
            print("‚ùå –ß–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return
        
        print(f"üìÑ –ü–µ—Ä–≤—ã–µ 10 —á–∞–Ω–∫–æ–≤ –ø–æ –ø–æ—Ä—è–¥–∫—É:")
        
        for chunk in chunks:
            document = db.query(Document).filter(Document.id == chunk.document_id).first()
            
            print(f"\nüß© –ß–∞–Ω–∫ {chunk.chunk_index} (ID: {chunk.id})")
            print(f"   –î–æ–∫—É–º–µ–Ω—Ç: {document.original_filename if document else f'ID {chunk.document_id}'}")
            print(f"   –†–∞–∑–º–µ—Ä: {chunk.content_length} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: '{chunk.content[:100]}{'...' if len(chunk.content) > 100 else ''}'")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        db.close()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ß–ê–ù–ö–û–í\n")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã
    chunks = analyze_chunk_sizes()
    
    if chunks:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
        show_large_chunks()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
        show_small_chunks()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏
        analyze_sequential_chunks()
    
    print("\n" + "="*60)
    print("üìä –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
    print("="*60)

if __name__ == "__main__":
    main() 