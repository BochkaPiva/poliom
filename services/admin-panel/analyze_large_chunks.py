#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
"""

import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ services
services_path = Path(__file__).parent.parent
sys.path.append(str(services_path))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
from dotenv import load_dotenv
load_dotenv('.env.local')

from sqlalchemy.orm import sessionmaker
from shared.models.database import engine
from shared.models import Document, DocumentChunk

# –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def analyze_chunk_sizes():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤"""
    print("üìä –ê–ù–ê–õ–ò–ó –†–ê–ó–ú–ï–†–û–í –ß–ê–ù–ö–û–í")
    print("=" * 60)
    
    db = SessionLocal()
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
        chunks = db.query(DocumentChunk).all()
        
        if not chunks:
            print("‚ùå –ß–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            return
        
        print(f"üì¶ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        print()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã
        sizes = [len(chunk.content) for chunk in chunks]
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_size = sum(sizes)
        min_size = min(sizes)
        max_size = max(sizes)
        avg_size = total_size / len(sizes)
        
        print("üìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ê–ó–ú–ï–†–û–í:")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π: {min_size} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π: {max_size} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –°—Ä–µ–¥–Ω–∏–π: {avg_size:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:,} —Å–∏–º–≤–æ–ª–æ–≤")
        print()
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º
        ranges = [
            (0, 100, "–û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ (0-100)"),
            (101, 300, "–ú–∞–ª–µ–Ω—å–∫–∏–µ (101-300)"),
            (301, 500, "–ù–µ–±–æ–ª—å—à–∏–µ (301-500)"),
            (501, 800, "–°—Ä–µ–¥–Ω–∏–µ (501-800)"),
            (801, 1000, "–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ (801-1000)"),
            (1001, 1500, "–ë–æ–ª—å—à–∏–µ (1001-1500)"),
            (1501, float('inf'), "–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ (1501+)")
        ]
        
        print("üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –†–ê–ó–ú–ï–†–ê–ú:")
        for min_r, max_r, label in ranges:
            count = len([s for s in sizes if min_r <= s <= max_r])
            percentage = (count / len(sizes)) * 100
            print(f"   {label}: {count} —á–∞–Ω–∫–æ–≤ ({percentage:.1f}%)")
        
        print()
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        print("üìÑ –ê–ù–ê–õ–ò–ó –ü–û –î–û–ö–£–ú–ï–ù–¢–ê–ú:")
        documents = db.query(Document).all()
        
        for doc in documents:
            doc_chunks = [chunk for chunk in chunks if chunk.document_id == doc.id]
            if doc_chunks:
                doc_sizes = [len(chunk.content) for chunk in doc_chunks]
                doc_min = min(doc_sizes)
                doc_max = max(doc_sizes)
                doc_avg = sum(doc_sizes) / len(doc_sizes)
                
                print(f"   üìÑ {doc.original_filename}:")
                print(f"      –ß–∞–Ω–∫–æ–≤: {len(doc_chunks)}")
                print(f"      –†–∞–∑–º–µ—Ä—ã: –º–∏–Ω={doc_min}, –º–∞–∫—Å={doc_max}, —Å—Ä–µ–¥–Ω–∏–π={doc_avg:.1f}")
        
        print()
        
        # –ü–æ–∏—Å–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        print("‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ù–´–ï –ß–ê–ù–ö–ò:")
        
        # –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
        small_chunks = [chunk for chunk in chunks if len(chunk.content) < 100]
        if small_chunks:
            print(f"   üìâ –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ (<100 —Å–∏–º–≤–æ–ª–æ–≤): {len(small_chunks)}")
            for chunk in small_chunks[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                preview = chunk.content[:50].replace('\n', ' ')
                print(f"      ID {chunk.id}: {len(chunk.content)} —Å–∏–º–≤–æ–ª–æ–≤ - '{preview}...'")
        
        # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
        large_chunks = [chunk for chunk in chunks if len(chunk.content) > 1500]
        if large_chunks:
            print(f"   üìà –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ (>1500 —Å–∏–º–≤–æ–ª–æ–≤): {len(large_chunks)}")
            for chunk in large_chunks[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                preview = chunk.content[:50].replace('\n', ' ')
                print(f"      ID {chunk.id}: {len(chunk.content)} —Å–∏–º–≤–æ–ª–æ–≤ - '{preview}...'")
        
        if not small_chunks and not large_chunks:
            print("   ‚úÖ –ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        print()
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        
        small_percentage = (len(small_chunks) / len(chunks)) * 100
        large_percentage = (len(large_chunks) / len(chunks)) * 100
        optimal_percentage = (len([s for s in sizes if 500 <= s <= 1000]) / len(sizes)) * 100
        
        if small_percentage > 10:
            print(f"   ‚ö†Ô∏è –ú–Ω–æ–≥–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ ({small_percentage:.1f}%) - –≤–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä")
        
        if large_percentage > 5:
            print(f"   ‚ö†Ô∏è –ú–Ω–æ–≥–æ –±–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤ ({large_percentage:.1f}%) - –≤–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–∑–±–∏–µ–Ω–∏—è")
        
        if optimal_percentage > 70:
            print(f"   ‚úÖ –•–æ—Ä–æ—à–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ ({optimal_percentage:.1f}% –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ)")
        else:
            print(f"   ‚ö†Ô∏è –¢–æ–ª—å–∫–æ {optimal_percentage:.1f}% —á–∞–Ω–∫–æ–≤ –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (500-1000 —Å–∏–º–≤–æ–ª–æ–≤)")
        
        if avg_size < 500:
            print("   üí° –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –º–∞–ª - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ chunk_size")
        elif avg_size > 1200:
            print("   üí° –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤–µ–ª–∏–∫ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ chunk_size")
        else:
            print("   ‚úÖ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ –Ω–æ—Ä–º–µ")
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
    
    finally:
        db.close()

def analyze_document_chunks(document_id: int):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print(f"üîç –ê–ù–ê–õ–ò–ó –ß–ê–ù–ö–û–í –î–û–ö–£–ú–ï–ù–¢–ê ID {document_id}")
    print("=" * 60)
    
    db = SessionLocal()
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        document = db.query(Document).filter(Document.id == document_id).first()
        if not document:
            print(f"‚ùå –î–æ–∫—É–º–µ–Ω—Ç —Å ID {document_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {document.original_filename}")
        print()
        
        # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        chunks = db.query(DocumentChunk).filter(DocumentChunk.document_id == document_id).all()
        
        if not chunks:
            print("‚ùå –ß–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        print(f"üì¶ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã
        sizes = [len(chunk.content) for chunk in chunks]
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤:")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π: {min(sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π: {max(sizes)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –°—Ä–µ–¥–Ω–∏–π: {sum(sizes)/len(sizes):.1f} —Å–∏–º–≤–æ–ª–æ–≤")
        print()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏
        print("üìã –°–ü–ò–°–û–ö –ß–ê–ù–ö–û–í:")
        for i, chunk in enumerate(chunks):
            size = len(chunk.content)
            preview = chunk.content[:80].replace('\n', ' ')
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ä–∞–∑–º–µ—Ä–∞
            if size < 300:
                category = "üìâ –ú–∞–ª–µ–Ω—å–∫–∏–π"
            elif size > 1200:
                category = "üìà –ë–æ–ª—å—à–æ–π"
            else:
                category = "‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω—ã–π"
            
            print(f"   {i+1:3d}. [{size:4d} —Å–∏–º–≤–æ–ª–æ–≤] {category}")
            print(f"        '{preview}...'")
            print()
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}")
    
    finally:
        db.close()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤")
    parser.add_argument("--doc-id", type=int, help="ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    args = parser.parse_args()
    
    if args.doc_id:
        analyze_document_chunks(args.doc_id)
    else:
        analyze_chunk_sizes() 