#!/usr/bin/env python3
"""
–¢–µ—Å—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
"""

import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ services
services_path = Path(__file__).parent.parent
sys.path.append(str(services_path))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
from dotenv import load_dotenv
load_dotenv('.env.local')

from sqlalchemy.orm import sessionmaker
from shared.models.database import engine
from shared.models import Document, DocumentChunk
from shared.utils.embeddings import EmbeddingService

# –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def test_semantic_search(query: str, top_k: int = 5):
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
    print(f"üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: '{query}'")
    print("-" * 60)
    
    db = SessionLocal()
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embedding_service = EmbeddingService()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        print("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...")
        query_embedding = embedding_service.get_embedding(query)
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {len(query_embedding)} –∏–∑–º–µ—Ä–µ–Ω–∏–π")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        print(f"üìä –ò—â–µ–º {top_k} –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤...")
        
        # SQL –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        sql_query = """
        SELECT 
            dc.id,
            dc.document_id,
            dc.chunk_index,
            dc.content,
            dc.content_length,
            d.original_filename,
            1 - (dc.embedding <=> %s::vector) as similarity
        FROM document_chunks dc
        JOIN documents d ON dc.document_id = d.id
        WHERE d.processing_status = 'completed'
        ORDER BY dc.embedding <=> %s::vector
        LIMIT %s
        """
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        result = db.execute(sql_query, (query_embedding, query_embedding, top_k))
        results = result.fetchall()
        
        if not results:
            print("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print()
        
        for i, row in enumerate(results, 1):
            chunk_id, doc_id, chunk_idx, content, content_len, filename, similarity = row
            
            print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç {i}:")
            print(f"   –î–æ–∫—É–º–µ–Ω—Ç: {filename}")
            print(f"   –ß–∞–Ω–∫: {chunk_idx + 1} (ID: {chunk_id})")
            print(f"   –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f}")
            print(f"   –†–∞–∑–º–µ—Ä: {content_len} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content[:200]}...")
            print()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
    finally:
        db.close()

def test_document_statistics():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("=" * 60)
    
    db = SessionLocal()
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
        documents = db.query(Document).all()
        
        print(f"üìö –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
        print()
        
        for doc in documents:
            print(f"üìÑ {doc.original_filename}")
            print(f"   ID: {doc.id}")
            print(f"   –°—Ç–∞—Ç—É—Å: {doc.processing_status}")
            print(f"   –ß–∞–Ω–∫–æ–≤: {doc.chunks_count or 0}")
            print(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {doc.file_size} –±–∞–π—Ç")
            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω: {doc.created_at}")
            if doc.processed_at:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω: {doc.processed_at}")
            print()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞–Ω–∫–æ–≤
        chunk_stats = db.execute("""
            SELECT 
                COUNT(*) as total_chunks,
                MIN(content_length) as min_size,
                MAX(content_length) as max_size,
                ROUND(AVG(content_length)) as avg_size
            FROM document_chunks
        """).fetchone()
        
        if chunk_stats:
            total, min_size, max_size, avg_size = chunk_stats
            print(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞–Ω–∫–æ–≤:")
            print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total}")
            print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min_size} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max_size} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_size} —Å–∏–º–≤–æ–ª–æ–≤")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
    finally:
        db.close()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    test_document_statistics()
    print("\n" + "="*80 + "\n")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –ø–æ–º–æ—â—å —Ä–∞–±–æ—Ç–Ω–∏–∫–∞–º",
        "–æ—Ç–ø—É—Å–∫ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–Ω–∏ –æ—Ç–¥—ã—Ö–∞",
        "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤",
        "–æ–ø–ª–∞—Ç–∞ —Ç—Ä—É–¥–∞ –∏ –∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞",
        "–ø—É—Ç–µ–≤–∫–∏ –≤ —Å–∞–Ω–∞—Ç–æ—Ä–∏–π"
    ]
    
    for query in test_queries:
        test_semantic_search(query, top_k=3)
        print("\n" + "="*80 + "\n")
    
    print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main() 