#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–æ–≤–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from sqlalchemy import text, create_engine

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
current_dir = Path(__file__).parent
load_dotenv(current_dir / '.env.local')

# –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
DATABASE_URL = os.getenv("DATABASE_URL")
engine = create_engine(DATABASE_URL)

def test_document_content():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    print("=" * 60)
    
    with engine.connect() as conn:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ (ID 8)
        doc_result = conn.execute(
            text("SELECT id, title, original_filename, processing_status, chunks_count FROM documents WHERE id = 8")
        )
        doc_info = doc_result.fetchone()
        
        if not doc_info:
            print("‚ùå –ù–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        doc_id, title, filename, status, chunks_count = doc_info
        print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {title}")
        print(f"   –§–∞–π–ª: {filename}")
        print(f"   –°—Ç–∞—Ç—É—Å: {status}")
        print(f"   –ß–∞–Ω–∫–æ–≤: {chunks_count}")
        print()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —á–∞–Ω–∫–æ–≤ (–±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
        chunks_result = conn.execute(
            text("""
                SELECT chunk_index, content, content_length 
                FROM document_chunks 
                WHERE document_id = :doc_id 
                ORDER BY chunk_index 
                LIMIT 5
            """),
            {"doc_id": doc_id}
        )
        
        print("üìù –ü–µ—Ä–≤—ã–µ 5 —á–∞–Ω–∫–æ–≤:")
        print("-" * 60)
        
        for chunk in chunks_result:
            chunk_index, content, length = chunk
            preview = content[:200] + "..." if len(content) > 200 else content
            print(f"–ß–∞–Ω–∫ {chunk_index} (–¥–ª–∏–Ω–∞: {length}):")
            print(f"   {preview}")
            print()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = ["–ø—Ä–µ–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ø—Ä–µ–º–∏—è", "—Ä–∞–±–æ—Ç–Ω–∏–∫", "—Å–æ—Ç—Ä—É–¥–Ω–∏–∫", "–≤—ã–ø–ª–∞—Ç–∞"]
        print("üîç –ü–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:")
        print("-" * 60)
        
        for keyword in keywords:
            search_result = conn.execute(
                text("""
                    SELECT COUNT(*) as count, 
                           STRING_AGG(SUBSTRING(content, 1, 100), ' | ') as examples
                    FROM document_chunks 
                    WHERE document_id = :doc_id 
                    AND LOWER(content) LIKE LOWER(:keyword)
                """),
                {"doc_id": doc_id, "keyword": f"%{keyword}%"}
            )
            
            result = search_result.fetchone()
            count, examples = result
            
            if count > 0:
                print(f"‚úÖ '{keyword}': –Ω–∞–π–¥–µ–Ω–æ –≤ {count} —á–∞–Ω–∫–∞—Ö")
                if examples:
                    print(f"   –ü—Ä–∏–º–µ—Ä—ã: {examples[:200]}...")
            else:
                print(f"‚ùå '{keyword}': –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            print()

def test_search_readiness():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –ø–æ–∏—Å–∫—É"""
    print("üöÄ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –ø–æ–∏—Å–∫—É")
    print("=" * 60)
    
    with engine.connect() as conn:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embedding_result = conn.execute(
            text("""
                SELECT COUNT(*) as total_chunks,
                       COUNT(embedding) as chunks_with_embeddings
                FROM document_chunks 
                WHERE document_id = 8
            """)
        )
        
        result = embedding_result.fetchone()
        total, with_embeddings = result
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:")
        print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total}")
        print(f"   –° —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {with_embeddings}")
        
        if total == with_embeddings:
            print("‚úÖ –í—Å–µ —á–∞–Ω–∫–∏ –∏–º–µ—é—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ - –≥–æ—Ç–æ–≤ –∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É –ø–æ–∏—Å–∫—É!")
        else:
            print(f"‚ö†Ô∏è  {total - with_embeddings} —á–∞–Ω–∫–æ–≤ –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        
        print()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        size_result = conn.execute(
            text("""
                SELECT 
                    MIN(content_length) as min_size,
                    MAX(content_length) as max_size,
                    ROUND(AVG(content_length)) as avg_size
                FROM document_chunks 
                WHERE document_id = 8
            """)
        )
        
        size_info = size_result.fetchone()
        min_size, max_size, avg_size = size_info
        
        print(f"üìè –†–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤:")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π: {min_size} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π: {max_size} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –°—Ä–µ–¥–Ω–∏–π: {avg_size} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if 500 <= avg_size <= 1000:
            print("‚úÖ –†–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞!")
        else:
            print("‚ö†Ô∏è  –†–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏")

def main():
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–û–í–û–ì–û –î–û–ö–£–ú–ï–ù–¢–ê")
    print("=" * 60)
    print()
    
    try:
        test_document_content()
        print()
        test_search_readiness()
        print()
        print("üéâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

if __name__ == "__main__":
    main() 