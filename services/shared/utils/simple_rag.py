# services/shared/utils/simple_rag.py

import logging
import numpy as np
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
from sqlalchemy.orm import Session
from sqlalchemy import text

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π
try:
    from services.shared.models.document import Document, DocumentChunk
except ImportError:
    # Fallback –¥–ª—è —Å–ª—É—á–∞—è, –µ—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω
    from models.document import Document, DocumentChunk

from .llm_client import SimpleLLMClient, LLMResponse

logger = logging.getLogger(__name__)

class SimpleRAG:
    """
    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–∞—è RAG —Å–∏—Å—Ç–µ–º–∞
    - –õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
    - GigaChat –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
    - –ù–∏–∫–∞–∫–∏—Ö —Å–ª–æ–∂–Ω–æ—Å—Ç–µ–π!
    """
    
    def __init__(self, db_session: Session, gigachat_api_key: str):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã
        
        Args:
            db_session: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            gigachat_api_key: API –∫–ª—é—á –¥–ª—è GigaChat
        """
        self.db_session = db_session
        self.llm_client = SimpleLLMClient(gigachat_api_key)
        self.similarity_threshold = 0.5  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–≥–µ—Ä
        self.logger = logging.getLogger(__name__)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        self.embedding_model = SentenceTransformer('cointegrated/rubert-tiny2')
        self.logger.info("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        
    def create_embedding(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            embedding = self.embedding_model.encode([text])[0]
            return embedding.tolist()
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return []
    
    def search_relevant_chunks(self, question: str, limit: int = 15) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞)
        
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # 1. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
            question_embedding = self.create_embedding(question)
            self.logger.info(f"–°–æ–∑–¥–∞–Ω —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(question_embedding)}")
            
            # 2. –í—ã–ø–æ–ª–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            self.logger.info("–ü—ã—Ç–∞–µ–º—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º pgvector –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            query = text("""
                SELECT dc.id, dc.document_id, dc.chunk_index, dc.content,
                       1 - (dc.embedding <=> :embedding) as similarity,
                       dc.content_length
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.processing_status = 'completed'
                  AND dc.embedding IS NOT NULL
                  AND dc.content_length > 100
                ORDER BY dc.embedding <=> :embedding
                LIMIT :limit
            """)
            
            result = self.db_session.execute(query, {
                'embedding': str(question_embedding),
                'limit': limit * 3  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            })
            
            vector_chunks = []
            for row in result:
                if row.similarity > 0.55:  # –ü–æ–≤—ã—à–∞–µ–º –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                    vector_chunks.append({
                        'id': row.id,
                        'document_id': row.document_id,
                        'chunk_index': row.chunk_index,
                        'content': row.content,
                        'similarity': row.similarity,
                        'search_type': 'vector',
                        'content_length': row.content_length
                    })
            
            self.logger.info(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(vector_chunks)} —á–∞–Ω–∫–æ–≤")
            
            # 3. –î–æ–ø–æ–ª–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–æ–∏—Å–∫–æ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –¥–∞–ª –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            text_chunks = []
            
            if len(vector_chunks) < limit // 2:  # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –º–µ–Ω—å—à–µ –ø–æ–ª–æ–≤–∏–Ω—ã –æ—Ç –ª–∏–º–∏—Ç–∞
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
                keywords = self._extract_keywords(question)
                
                if keywords:
                    self.logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {keywords}")
                    
                    # –°—Ç—Ä–æ–∏–º –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    conditions = []
                    params = {}
                    
                    for i, keyword in enumerate(keywords):
                        param_name = f'keyword_{i}'
                        conditions.append(f"dc.content ILIKE :{param_name}")
                        params[param_name] = f'%{keyword}%'
                    
                    text_query = text(f"""
                        SELECT DISTINCT dc.id, dc.document_id, dc.chunk_index, dc.content,
                               0.7 as similarity, dc.content_length
                        FROM document_chunks dc
                        JOIN documents d ON dc.document_id = d.id
                        WHERE d.processing_status = 'completed'
                          AND dc.content_length > 100
                          AND ({' OR '.join(conditions)})
                        LIMIT :limit
                    """)
                    
                    params['limit'] = limit
                    text_result = self.db_session.execute(text_query, params)
                    
                    for row in text_result:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ—Ç —á–∞–Ω–∫ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
                        if not any(chunk['id'] == row.id for chunk in vector_chunks):
                            text_chunks.append({
                                'id': row.id,
                                'document_id': row.document_id,
                                'chunk_index': row.chunk_index,
                                'content': row.content,
                                'similarity': row.similarity,
                                'search_type': 'text',
                                'content_length': row.content_length
                            })
                    
                    self.logger.info(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(text_chunks)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            
            # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            all_chunks = vector_chunks + text_chunks
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
            all_chunks.sort(key=lambda x: x['similarity'], reverse=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_chunks = all_chunks[:limit]
            
            if not final_chunks:
                self.logger.info("–í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return self._fallback_search(question, limit)
            
            return final_chunks
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ search_relevant_chunks: {str(e)}")
            return self._fallback_search(question, limit)
    
    def _extract_keywords(self, question: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        synonyms = {
            '–∞–≤–∞–Ω—Å': ['–∞–≤–∞–Ω—Å', '–∞–≤–∞–Ω—Å–æ–≤–∞—è', '–∞–≤–∞–Ω—Å–æ–≤—ã–π', '–ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å', '–ø–µ—Ä–≤–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞', '–ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞'],
            '–∑–∞—Ä–ø–ª–∞—Ç–∞': ['–∑–∞—Ä–ø–ª–∞—Ç–∞', '–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞ —Ç—Ä—É–¥–∞', '–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ', '–∑–ø'],
            '–≤—ã–ø–ª–∞—Ç–∞': ['–≤—ã–ø–ª–∞—Ç–∞', '–≤—ã–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è', '–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ', '–Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ', '–≤—ã–¥–∞—á–∞'],
            '–¥–∞—Ç–∞': ['–¥–∞—Ç–∞', '—á–∏—Å–ª–æ', '—Å—Ä–æ–∫', '–≤—Ä–µ–º—è', '–∫–æ–≥–¥–∞', '–¥–µ–Ω—å'],
            '—Ä–∞–∑–º–µ—Ä': ['—Ä–∞–∑–º–µ—Ä', '—Å—É–º–º–∞', '–ø—Ä–æ—Ü–µ–Ω—Ç', '—Å–∫–æ–ª—å–∫–æ', '–≤–µ–ª–∏—á–∏–Ω–∞'],
            '–æ—Ç–ø—É—Å–∫': ['–æ—Ç–ø—É—Å–∫', '–æ—Ç–ø—É—Å–∫–Ω—ã–µ', '–æ—Ç–¥—ã—Ö', '–∫–∞–Ω–∏–∫—É–ª—ã'],
            '–±–æ–ª—å–Ω–∏—á–Ω—ã–π': ['–±–æ–ª—å–Ω–∏—á–Ω—ã–π', '–±–æ–ª–µ–∑–Ω—å', '–Ω–µ—Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å', '–ª–∏—Å—Ç –Ω–µ—Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏'],
            '–ø—Ä–µ–º–∏—è': ['–ø—Ä–µ–º–∏—è', '–±–æ–Ω—É—Å', '–ø–æ–æ—â—Ä–µ–Ω–∏–µ', '–Ω–∞–¥–±–∞–≤–∫–∞'],
            '–¥–æ–≥–æ–≤–æ—Ä': ['–¥–æ–≥–æ–≤–æ—Ä', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '—Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä'],
            '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ': ['—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ', '—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ', '–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ', '—É—Ö–æ–¥'],
            '–≥—Ä–∞—Ñ–∏–∫': ['–≥—Ä–∞—Ñ–∏–∫', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '—Ä–µ–∂–∏–º', '–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã'],
            '–¥–æ–∫—É–º–µ–Ω—Ç—ã': ['–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Å–ø—Ä–∞–≤–∫–∏', '–±—É–º–∞–≥–∏', '—Ñ–æ—Ä–º—ã']
        }
        
        question_lower = question.lower()
        keywords = set()
        
        # –ò—â–µ–º –ø—Ä—è–º—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        for base_word, word_list in synonyms.items():
            for word in word_list:
                if word in question_lower:
                    keywords.update([base_word])  # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤–æ–µ —Å–ª–æ–≤–æ
                    keywords.add(word)  # –ò —Å–∞–º–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ
                    break
        
        # –î–æ–±–∞–≤–ª—è–µ–º —á–∏—Å–ª–∞ (–¥–∞—Ç—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã)
        import re
        numbers = re.findall(r'\b\d{1,2}\b', question)
        for num in numbers:
            keywords.add(num)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', question_lower)
        stop_words = {'–∫–æ–≥–¥–∞', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '—Å–∫–æ–ª—å–∫–æ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–æ—Ç–∫—É–¥–∞', '–∫—É–¥–∞'}
        for word in words:
            if word not in stop_words:
                keywords.add(word)
        
        return list(keywords)[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    def _fallback_search(self, question: str, limit: int) -> List[Dict]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            self.logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ fallback")
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            query = text("""
                SELECT dc.id, dc.document_id, dc.chunk_index, dc.content,
                       0.5 as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.processing_status = 'completed'
                  AND (dc.content ILIKE :search_term1 
                       OR dc.content ILIKE :search_term2
                       OR dc.content ILIKE :search_term3)
                LIMIT :limit
            """)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
            words = question.lower().split()
            search_terms = [f'%{word}%' for word in words if len(word) > 2][:3]
            
            if not search_terms:
                return []
            
            params = {
                'search_term1': search_terms[0] if len(search_terms) > 0 else '%',
                'search_term2': search_terms[1] if len(search_terms) > 1 else search_terms[0],
                'search_term3': search_terms[2] if len(search_terms) > 2 else search_terms[0],
                'limit': limit
            }
            
            result = self.db_session.execute(query, params)
            
            chunks = []
            for row in result:
                chunks.append({
                    'id': row.id,
                    'document_id': row.document_id,
                    'chunk_index': row.chunk_index,
                    'content': row.content,
                    'similarity': row.similarity,
                    'search_type': 'fallback'
                })
            
            self.logger.info(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            return chunks
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ fallback –ø–æ–∏—Å–∫–µ: {str(e)}")
            return []
    
    def format_context(self, chunks: List[DocumentChunk]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        if not chunks:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document = self.db_session.query(Document).filter(
                Document.id == chunk['document_id']
            ).first()
            
            doc_title = document.title if document else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"
            
            context_parts.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {doc_title}]\n{chunk['content']}\n"
            )
        
        return "\n".join(context_parts)
    
    def answer_question(self, 
                       question: str,
                       user_id: Optional[int] = None) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            
        Returns:
            Dict —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            self.logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å: {question[:100]}...")
            
            # 1. –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            relevant_chunks = self.search_relevant_chunks(question)
            
            if not relevant_chunks:
                return {
                    'answer': '–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ HR-–æ—Ç–¥–µ–ª—É.',
                    'sources': [],
                    'success': True,
                    'tokens_used': 0
                }
            
            # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = self.format_context(relevant_chunks)
            
            # –û–¢–õ–ê–î–ö–ê: –í—ã–≤–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ª–æ–≥
            self.logger.info(f"üîç –ö–û–ù–¢–ï–ö–°–¢ –î–õ–Ø LLM (–¥–ª–∏–Ω–∞: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤):")
            self.logger.info("="*80)
            self.logger.info(context[:1000] + "..." if len(context) > 1000 else context)
            self.logger.info("="*80)
            
            # 3. –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç LLM
            llm_response = self.llm_client.generate_answer(
                context=context,
                question=question
            )
            
            if not llm_response.success:
                return {
                    'answer': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.',
                    'sources': [],
                    'success': False,
                    'error': llm_response.error,
                    'tokens_used': 0
                }
            
            # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π
            sources = []
            seen_documents = set()
            
            for chunk in relevant_chunks:
                document = self.db_session.query(Document).filter(
                    Document.id == chunk['document_id']
                ).first()
                
                if document and document.title not in seen_documents:
                    sources.append({
                        'title': document.title,
                        'chunk_index': chunk['chunk_index'],
                        'document_id': document.id
                    })
                    seen_documents.add(document.title)
            
            # 5. –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            formatted_answer = self._post_process_answer(llm_response.text)

            # 6. –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if user_id:
                self._log_query(user_id, question, formatted_answer, len(relevant_chunks))

            return {
                'answer': formatted_answer,
                'sources': sources,
                'success': True,
                'tokens_used': llm_response.tokens_used,
                'chunks_found': len(relevant_chunks)
            }
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ answer_question: {str(e)}")
            return {
                'answer': '–ü—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.',
                'sources': [],
                'success': False,
                'error': str(e),
                'tokens_used': 0
            }
    
    def _post_process_answer(self, answer: str) -> str:
        """
        –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            answer: –ò—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM
            
        Returns:
            str: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è Telegram MarkdownV2
        # –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞—Ç—å: _ * [ ] ( ) ~ ` > # + - = | { } . !
        escape_chars = ['_', '*', '[', ']', '(', ')', '~', '`', '>', '#', '+', '-', '=', '|', '{', '}', '.', '!']
        
        for char in escape_chars:
            answer = answer.replace(char, f'\\{char}')
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        answer = answer.strip()
        
        # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        lines = answer.split('\n')
        unique_lines = []
        seen_lines = set()
        
        for line in lines:
            line_clean = line.strip().lower()
            if line_clean and line_clean not in seen_lines:
                unique_lines.append(line)
                seen_lines.add(line_clean)
        
        return '\n'.join(unique_lines)
    
    def _log_query(self, user_id: int, question: str, answer: str, chunks_count: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            from shared.models.query_log import QueryLog
            
            log_entry = QueryLog(
                user_id=user_id,
                query_text=question,
                response_text=answer,
                chunks_used=chunks_count,
                model_used="GigaChat"
            )
            
            self.db_session.add(log_entry)
            self.db_session.commit()
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
    
    def health_check(self) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        return {
            'embeddings_model': self.embedding_model is not None,
            'llm_client': self.llm_client.health_check(),
            'database': self._check_database()
        }
    
    def _check_database(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            self.db_session.execute(text("SELECT 1"))
            return True
        except Exception:
            return False 